---
title: 'Sampling Distributions'
author: Ben Drucker
bibliography: bib.bib
csl: NumberedFormat.csl
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: show
    css: stat21-lab-theme.css
  pdf_document:
    toc: yes
nocite: |
  @veaux2012stats

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(tidymodels)
library(patchwork)
library(dials)
library(ggplot2)
theme_set(theme_minimal(base_family = "Georgia"))
```

### Introduction and Motivation

Many times, we are interested in inferring information about a population using only information from a sample. These sorts of studies tend to proportions or means of some quality of the population. For example, we might wonder if the proportion of stay-at-home fathers is 50% (i.e., is there an even gender split in stay-at-home parents[^gender-simplicity])? Or, we might ask if there is a difference in the mean household income between men in heterosexual marriages vs. in homosexual marriages. (Both of these questions come from [this]() Pew study (@barroso_fry_2021).)

For these types of questions, it would be completely impractical to survey every household. However, it *is* practical (and common) to survey a limited subset (**sample**) of the population in question. Obviously, it is unlikely for the answer to our question to be found *exactly*, using a sample—after all, we are not surveying the whole population! But what does it take to get an answer that is "close enough" to the exact answer? How many people do we need to survey? What assumptions about our data need to be met to accept our "close enough answer" as truly being "close enough?"

Read on to learn!

### Sampling distributions for Proportions

Let's begin a simple example: marbles in a jar. Suppose I have a jar filled with 500,000 blue marbles and 500,000 red marbles. Thus, there are one million marbles in total. However, suppose I don't tell you that and I task you with estimating the proportion of red marbles in the jar. It would be impractical to count all the marbles, so you decide to count a sample of them and use that as the estimate. (This is analogous to the example about stay-at-home parents earlier!)

#### **Q1:** The code below simulates selections from the jar with samples of size 3, 10, 100, and 1,000, and displays the results in histograms. Before running this code, what do you predict will happen when sample size increases. After running the code, was your hypothesis true?

```{r}
marbles <- c()

for (i in 1:10000) {
  marbles <- append(marbles, "red")
}

for (i in 1:10000) {
  marbles <- append(marbles, "blue")
}

size3 <- tibble(marbles3 = sample(marbles, 3))
size10 <- tibble(marbles10 = sample(marbles, 10))
size100 <- tibble(marbles100 = sample(marbles, 100))
size1000 <- tibble(marbles1000 = sample(marbles, 1000))

marble_plot = ggplot(size3) +
  geom_bar(mapping = aes(x = marbles3)) +
ggplot(size10) +
  geom_bar(mapping = aes(x = marbles10)) +
ggplot(size100) +
  geom_bar(mapping = aes(x = marbles100)) +
ggplot(size1000) +
  geom_bar(mapping = aes(x = marbles1000))

# UNCOMMENT TO SHOW RESULTING PLOT
# marble_plot
```

You should have found that as the sample size increases, the bar graphs are generally less variable (across different runs of the code) and closer to the true proportion of blue/red marbles in the jar.

What if we were to rerun this sampling procedure many times (with a fixed sample size), counted the proportion of blue marbles each time, and plotted the resulting proportions on a histogram?  

#### **Q2:** The following code chooses 1,000 samples from the marble jar, each of size 1000. What might you imagine this histogram will look like? What will its center be? What shape will it be? Try running this code multiple times, with different sample sizes. What do you observe?

```{r}
v <- c()

for (i in 1:1000) {
  size10 <- tibble(marbles10 = sample(marbles, 1000))
  filter(size10, marbles10 == "blue") %>%
    summarise(num_blue = n()) %>%
      select(num_blue) -> output
  
  v <- append(v, output[[1]]/1000)
}

data_ = tibble(count = v)

ggplot(data_) +
  geom_histogram(mapping = aes(x = count), binwidth = 2)
```

You should have observed that the distribution gets narrower and narrower as the sample size increases. This should intuitively make sense. As we increase sample size, it seems more likely that each sample is closer to the true proportion of blue marbles in the jar. 

This type of histogram is called a **sampling distribution of a proportion**. This is because it is the *distribution* of *samples* for which we are trying to gain information about a population *proportion*.

The following fact about sampling distributions is possibly less obvious: as sample size increases—the *normal model* becomes a better and better model for the sampling distribution. This is due to the **Central Limit Theorem**, a pivotal result in statistics. The fact specifically applying to sampling distributions is just one part of the more general result proven by Pierre-Simon Laplace in 1810.

Now that we know the sampling distribution follows a normal model[^cond], it is natural to ask about its center and spread. 

#### **Q3:** What do you think the center of the sampling distribution is? (As a challenge, you can also think about its standard deviation, but it is difficult to guess just by looking at a sampling distribution.) *Answer in the code chunk below.*

```{r}
  # As you can probably imagine,
  # sampling distributions for proportions are centered at p,
  # the true population proportion.
  # This should intuitively make sense; with many random samples,
  # we would expect the samples to average to the
  # underlying population proportion.
```

As mentioned earlier, standard deviation is trickier. To determine this quantity, we briefly step away from the normal model and turn to the underlying data. This follows a binomial model, which has standard deviation $\sqrt{np(1-p)}.$ This quantity is the standard deviation of the *number* of "successes" (blue marbles). However, we are interested in the standard deviation in the *proportion* of successes. Thus, we divide $\sqrt{np(1-p)}.$ by $n$ as follows:

$$\frac{\sqrt{np(1-p)}}{n} = \frac{\sqrt{np(1-p)}}{\sqrt{n^2}} = \sqrt{\frac{np(1-p)}{n^2}} = \sqrt{\frac{p(1-p)}{n}}.$$

Similarly, the center of the binomial distribution is the true number of successes.<!--FIXME--> Thus, the center of the sampling distribution is the true number of successes divided by $n$, or $p$. 

#### **Q4:** Calculate the mean and standard deviation for a sampling distribution of proportions with $p = 0.70$ and $n = 421$ *Answer in the code chunk below.*.

```{r}
# The answer is as follows:
mean = 0.70
SD = sqrt(0.7*0.3/421) # equals about 0.223
```

#### **Q5:** Suppose we think the proportion of blue marbles is 0.5. Would you be surprised if you took a sample of 1000 marbles and 51% were blue? What if 40% were blue? How might you quantify this answer? *Answer in the code chunk below.*

```{r}
# Let's think about the underlying sampling distribution.
# It is centered at the true proportion of blue marbles.
# Since we suppose that p = 0.5, the standard deviation of
# the sampling distribution is sqrt(0.5*0.5/1000) ≈ 0.0158.
# Thus, if we picked 51% blue marbles in our sample, the 1%
# increase over p represents 0.01/0.0158 ≈ 0.632 SDs above p.
# How surprising is this? The probability of getting a difference of
# 0.632 SDs above or below the mean of the distribution (p) is the
# area under the bell curve to the left and right of 0.632 SDs above
# and below the mean. Using R code, this probability is 
# 2*pnorm(-0.632), or about 52.7%. So getting 51% is not 
# that surprising. On the other hand, if we do a similar analysis with 
# the 40% case, the probability drops all the way to 0.00000000764% !
# So this would be extremely surprising. 

```

The reason we care about sampling distributions is that they allow us to consider the distribution of sample proportions across many different (theoretical) samples. Then, we can compare this distribution to our actual sample and check how unlikely the proportion of our sample is *Given that our hypothesis---our null hypothesis---about the distribution's center is true*. This is called **hypothesis testing**. 

<!--TODO If time, go into detail about the SD of the binomial model-->
<!--QQ: Sample distribution vs. sampling distribution: Which (if either) can mean a distribution of actual data? Which is just "all possible"?-->

**Q6: Read the following article on the study about stay-at-home parents mentioned earlier. We want to examine the sampling distribution for this study. (Let your null hypothesis be "the percentage of female stay-at-home parents is 50%.") First, draw out the sampling distribution indicating the center and standard deviation. Draw two vertical lines on the histogram corresponding to the sample proportion. Qualitatively, what do you conclude?**

<!--TODO: Answer-->

## Sampling distributions for means

This section will explore sampling distributions for *means*. In the last section, we looked at sampling distributions for proportions. 

To see that the underlying principles of sampling distributions hold, consider the following example. Suppose we again have 10000<!--TODO: fix 1M/10000--> marbles in a jar, but this time, the marbles' diameters are integers from 1 to 10 (inclusive). Suppose that the population mean of the marble diameters is 5cm. Again, supposing we don't know this, we want to estimate the mean marble diameter using a sample. 

<!--FIXME: Replacement question -->

#### **Q7:** The following code shows histograms of marble diameters found in random samples of differing sizes. It also shows the mean of the sample. Try running the code multiple times. Verify that the mean is more stable for samples of larger sizes.

```{r}
marbles <- c()
for (diam in 1:10) {
    for (i in 1:1000) {
    marbles <- append(marbles, diam)
  }
}
# TODO: make the histograms flush and with 10 bins
size3 <- tibble(marbles3 = sample(marbles, 3))
size10 <- tibble(marbles10 = sample(marbles, 10))
size100 <- tibble(marbles100 = sample(marbles, 100))
size1000 <- tibble(marbles1000 = sample(marbles, 1000))

size3 <- mutate(size3, mean_ = mean(marbles3), count_ = n()/2)
size10 <- mutate(size10, mean_ = mean(marbles10), count_ = n()/2)
size100 <- mutate(size100, mean_ = mean(marbles100), count_ = n()/2)
size1000 <- mutate(size1000, mean_ = mean(marbles1000), count_ = n()/2)

marble_plot_mean <- ggplot(size3) +
  geom_bar(mapping = aes(x = marbles3)) +
  geom_point(mapping = aes(x = mean_, y = count_),
  size = 10, color = "red", alpha = 0.5, shape = "square") +
ggplot(size10) +
  geom_bar(mapping = aes(x = marbles10)) +
  geom_point(mapping = aes(x = mean_, y = count_),
  size = 10, color = "red", alpha = 0.5, shape = "square") +
ggplot(size100) +
  geom_bar(mapping = aes(x = marbles100)) +
  geom_point(mapping = aes(x = mean_, y = count_),
  size = 10, color = "red", alpha = 0.5, shape = "square") +
ggplot(size1000) +
  geom_bar(mapping = aes(x = marbles1000)) +
  geom_point(mapping = aes(x = mean_, y = count_),
  size = 10, color = "red", alpha = 0.5, shape = "square")

# UNCOMMENT TO SHOW RESULTING PLOT
marble_plot_mean
```

[^gender-simplicity]: This question assumes a gender binary for simplicity, though this binary does not reflect the true range of genders.
[^cond]: (Under special conditions we will outline earlier)

### References

https://bookdown.org/yihui/rmarkdown-cookbook/bibliography.html
https://stat.ethz.ch/R-manual/R-devel/library/base/html/sample.html
https://www.datamentor.io/r-programming/for-loop/
https://www.w3schools.com/r/r_for_loop.asp
<!--https://stat.ethz.ch/R-manual/R-devel/library/base/html/Colon.html
https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/append-->
http://www.simonqueenborough.info/R/stats-advanced/sampling
https://www.tutorialspoint.com/r/r_binomial_distribution.htm
http://seankross.com/rbootcamp/simulation.html