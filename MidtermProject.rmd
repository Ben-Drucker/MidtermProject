---
title: 'Sampling Distributions'
author: Ben Drucker
bibliography: bib.bib
csl: NumberedFormat.csl
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
    css: stat21-lab-theme.css
  pdf_document:
    toc: yes
    extra_dependencies: ["url"]
nocite: |
  @veaux2012stats
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(tidymodels)
library(patchwork)
library(dials)
library(ggplot2)
theme_set(theme_minimal(base_family = "Georgia"))
```

### Introduction and Motivation

Many times, we are interested in inferring information about a population using only information from a sample. These sorts of studies tend to proportions or means of some quality of the population. For example, we might wonder if the proportion of female gun-owners is 50% (i.e., is there an even gender split in American gun ownership[^gender-simplicity])? Or, we might ask if there is a difference in the mean household income between men in heterosexual marriages vs. in homosexual marriages. (Both of these questions come from [this]() Pew study (@barroso_fry_2021).)

For these types of questions, it would be completely impractical to survey every household. However, it *is* practical (and common) to survey a limited subset (**sample**) of the population in question. Obviously, it is unlikely for the answer to our question to be found *exactly*, using a sample—after all, we are not surveying the whole population! But what does it take to get an answer that is "close enough" to the exact answer? How many people do we need to survey? What assumptions about our data need to be met to accept our "close enough answer" as truly being "close enough?"

Read on to learn!

### Sampling distributions for Proportions

Let's begin a simple example: marbles in a jar. Suppose I have a jar filled with 500,000 blue marbles and 500,000 red marbles. Thus, there are one million marbles in total. However, suppose I don't tell you that and I task you with estimating the proportion of red marbles in the jar. It would be impractical to count all the marbles, so you decide to count a sample of them and use that as the estimate. (This is analogous to the example about stay-at-home parents earlier!)

#### **Q1:** The code below[^doc] simulates selections from the jar with samples of size 3, 10, 100, and 1,000, and displays the results in histograms. Before running this code, what do you predict will happen when sample size increases. After running the code, was your hypothesis true?

```{r fig.show='hide'}
marbles <- c()

for (i in 1:10000) {
  marbles <- append(marbles, "red")
}

for (i in 1:10000) {
  marbles <- append(marbles, "blue")
}

size3 <- tibble(marbles3 = sample(marbles, 3))
size10 <- tibble(marbles10 = sample(marbles, 10))
size100 <- tibble(marbles100 = sample(marbles, 100))
size1000 <- tibble(marbles1000 = sample(marbles, 1000))

marble_plot = ggplot(size3) +
  geom_bar(mapping = aes(x = marbles3, fill = marbles3)) +
  scale_fill_manual(values = c("#0000ff6c", "#ff00006c")) +
  xlim("blue", "red") +
ggplot(size10) +
  geom_bar(mapping = aes(x = marbles10, fill = marbles10)) +
  scale_fill_manual(values = c("#0000ff6c", "#ff00006c")) +
  xlim("blue", "red") +
ggplot(size100) +
  geom_bar(mapping = aes(x = marbles100, fill = marbles100)) +
  scale_fill_manual(values = c("#0000ff6c", "#ff00006c")) +
  xlim("blue", "red") +
ggplot(size1000) +
  geom_bar(mapping = aes(x = marbles1000, fill = marbles1000)) +
  scale_fill_manual(values = c("#0000ff6c", "#ff00006c")) +
  xlim("blue", "red")

marble_plot
```

You should have found that as the sample size increases, the bar graphs are generally less variable (across different runs of the code) and closer to the true proportion of blue/red marbles in the jar.

What if we were to rerun this sampling procedure many times (with a fixed sample size), counted the proportion of blue marbles each time, and plotted the resulting proportions on a histogram?  

#### **Q2:** The following code chooses 1,000 samples from the marble jar, each of size 1000. What might you imagine this histogram will look like? What will its center be? What shape will it be? Try running this code multiple times, with different sample sizes. What do you observe?

```{r fig.show='hide'}
v <- c()

for (i in 1:1000) {
  size10 <- tibble(marbles10 = sample(marbles, 1000))
  filter(size10, marbles10 == "blue") %>%
    summarise(num_blue = n()) %>%
      select(num_blue) -> output
  
  v <- append(v, output[[1]]/1000)
}

data_ = tibble(count = v)

ggplot(data_) +
  geom_histogram(mapping = aes(x = count), binwidth = 2)
```

You should have observed that the distribution gets narrower and narrower as the sample size increases. This should intuitively make sense. As we increase sample size, it seems more likely that each sample is closer to the true proportion of blue marbles in the jar. 

This type of histogram is called a **sampling distribution of a proportion**. This is because it is the *distribution* of *samples* for which we are trying to gain information about a population *proportion*.

The following fact about sampling distributions is possibly less obvious: as sample size increases—the *normal model* becomes a better and better model for the sampling distribution. This is due to the **Central Limit Theorem**, a pivotal result in statistics. The fact specifically applying to sampling distributions is just one part of the more general result proven by Pierre-Simon Laplace in 1810.

Now that we know the sampling distribution follows a normal model[^cond], it is natural to ask about its center and spread. 

#### **Q3:** What do you think the center of the sampling distribution is? (As a challenge, you can also think about its standard deviation, but it is difficult to guess just by looking at a sampling distribution.) *Answer in the code chunk below.*

```{r}
  # As you can probably imagine,
  # sampling distributions for proportions are centered at p,
  # the true population proportion.
  # This should intuitively make sense; with many random samples,
  # we would expect the samples to average to the
  # underlying population proportion.
```

As mentioned earlier, standard deviation is trickier. To determine this quantity, we briefly step away from the normal model and turn to the underlying data. This follows a binomial model, which has standard deviation $\sqrt{np(1-p)}.$ This quantity is the standard deviation of the *number* of "successes" (blue marbles). However, we are interested in the standard deviation in the *proportion* of successes. Thus, we divide $\sqrt{np(1-p)}.$ by $n$ as follows:

$$\frac{\sqrt{np(1-p)}}{n} = \frac{\sqrt{np(1-p)}}{\sqrt{n^2}} = \sqrt{\frac{np(1-p)}{n^2}} = \sqrt{\frac{p(1-p)}{n}}.$$

Similarly, the center of the binomial distribution is the true number of successes.<!--FIXME--> Thus, the center of the sampling distribution is the true number of successes divided by $n$, or $p$. 

#### **Q4:** Calculate the mean and standard deviation for a sampling distribution of proportions with $p = 0.70$ and $n = 421$ *Answer in the code chunk below.*.

```{r results='hide'}
# The answer is as follows:
mean = 0.70
SD = sqrt(0.7*0.3/421) # equals about 0.223
```

#### **Q5:** Suppose we think the proportion of blue marbles is 0.5. Would you be surprised if you took a sample of 1000 marbles and 51% were blue? What if 40% were blue? How might you quantify this answer? *Answer in the code chunk below.*

```{r}
# Let's think about the underlying sampling distribution.
# It is centered at the true proportion of blue marbles.
# Since we suppose that p = 0.5, the standard deviation of
# the sampling distribution is sqrt(0.5*0.5/1000) ≈ 0.0158.
# Thus, if we picked 51% blue marbles in our sample, the 1%
# increase over p represents 0.01/0.0158 ≈ 0.632 SDs above p.
# How surprising is this? The probability of getting a difference of
# 0.632 SDs above or below the mean of the distribution (p) is the
# area under the bell curve to the left and right of 0.632 SDs above
# and below the mean. Using R code, this probability is 
# 2*pnorm(-0.632), or about 52.7%. So getting 51% is not 
# that surprising. On the other hand, if we do a similar analysis with 
# the 40% case, the probability drops all the way to 0.00000000764% !
# So this would be extremely surprising. 

```

The reason we care about sampling distributions is that they allow us to consider the distribution of sample proportions across many different (theoretical) samples. Then, we can compare this distribution to our actual sample and check how unlikely the proportion of our sample is *Given that our hypothesis---our null hypothesis---about the distribution's center is true*. This is called **hypothesis testing**. 

<!--TODO If time, go into detail about the SD of the binomial model-->
<!--QQ: Sample distribution vs. sampling distribution: Which (if either) can mean a distribution of actual data? Which is just "all possible"?-->

**Q6: Read the introduction to the following article on [https://news.gallup.com/poll/264932/percentage-americans-own-guns.aspx](the study about gun ownership mentioned earlier.) We want to examine the sampling distribution for this study. (Let your null hypothesis be "the difference in percentage of males vs. who are male vs. female is 0%.") First, draw out the sampling distribution indicating the center and standard deviation. Draw two vertical lines on the histogram corresponding to the sample proportion, $\hat{p}$. Qualitatively, what do you conclude?**

```{r}
# From the article, 62% of gun owners are male 
# (which, for simplicity means that 38% of gun owners are female.)
# Since our assumption was that there is no gender gap in gun ownership,
# our sampling distribution is centered at 0.5, and has a standard
# deviation of sqrt(0.5*0.5/1269) = 0.014 since the sample size was 1296.
# Graph below:
ggplot(data = data.frame(x = c(0.35, 0.65)), mapping =  aes(x = x)) +
  stat_function(fun = dnorm, args = list(0.5, 0.014)) +
  geom_vline(xintercept = 0.38, color = "red")

# Based on the graph, it is very unlikely that our
# sample proportion of 38% would have been chosen, given that
# we assume the true proportion of female gun owners is 50%.
```

### Sampling distributions for means

This section will explore sampling distributions for *means*. In the last section, we looked at sampling distributions for proportions. 

To see that the underlying principles of sampling distributions hold, consider the following example. Suppose we again have 10000<!--TODO: fix 1M/10000--> marbles in a jar, but this time, the marbles' diameters are integers from 1 to 9 (inclusive). Suppose that the population mean of the marble diameters is 5cm. Again, supposing we don't know this, we want to estimate the mean marble diameter using a sample. 

<!--FIXME: Replacement question -->

#### **Q7:** The following code shows histograms of marble diameters found in random samples of differing sizes. It also shows the mean of the sample. Try running the code multiple times. Verify that the mean is more stable for samples of larger sizes.

```{r fig.show='hide'}
marbles <- c()
for (diam in 1:9) {
    for (i in 1:1000) {
    marbles <- append(marbles, diam)
  }
}
# TODO: make the histograms flush and with 10 bins
size3 <- tibble(marbles3 = sample(marbles, 3))
size10 <- tibble(marbles10 = sample(marbles, 10))
size100 <- tibble(marbles100 = sample(marbles, 100))
size1000 <- tibble(marbles1000 = sample(marbles, 1000))

size3 <- mutate(size3, mean_ = mean(marbles3))
size10 <- mutate(size10, mean_ = mean(marbles10))
size100 <- mutate(size100, mean_ = mean(marbles100))
size1000 <- mutate(size1000, mean_ = mean(marbles1000))

marble_plot_mean <- ggplot(size3) +
  geom_histogram(mapping = aes(x = marbles3), bins = 10,
  boundary = 0) +
  xlim(0, 10) +
  geom_vline(aes(xintercept = mean_), color = "red") +
ggplot(size10) +
  geom_histogram(mapping = aes(x = marbles10), bins = 10,
  boundary = 0) +
  xlim(0, 10) +
  geom_vline(aes(xintercept = mean_), color = "red") +
ggplot(size100) +
  geom_histogram(mapping = aes(x = marbles100), bins = 10,
  boundary = 0) +
  xlim(0, 10) +
  geom_vline(aes(xintercept = mean_), color = "red") +
ggplot(size1000) +
  geom_histogram(mapping = aes(x = marbles1000), bins = 10,
  boundary = 0) +
  xlim(0, 10) +
  geom_vline(aes(xintercept = mean_), color = "red")
marble_plot_mean
```

As we did before, let's now automate this process and create a histogram of means across many different samples.

```{r fig.show = 'hide'}
v <- c()

for (i in 1:1000) {
  sample_ <- sample(marbles, 1000)
  v <- append(v, mean(sample_))
}

data_ <- tibble(mean_ = v)

ggplot(data_) +
  geom_histogram(mapping = aes(x = mean_)) +
  xlim(4.5, 5.5)
```

Yes! This distribution looks normal! What is its center and standard deviation? It should again feel natural the its center is $\mu$, the population mean. And just like before, standard deviation is also tricky. Here it is $\frac{\sigma}{\sqrt{n}}$, where $\sigma$ is the standard deviation of the *population*.
<!--TODO: If time, explain reason for this standard deviation.-->

**Q8:** Sometimes our sampling distribution is centered at zero. Why might this be? (Hint: What might we be trying to test with such a distribution) *Answer in code chuck below*

```{r}
# Our sampling distribution would be centered at zero if we
# are hypothesis testing a difference between two means or two proportions,
# and we hypothesize that the true difference is zero.
# One example is the study about incomes in marriages with different gender
# configurations mentioned in the introduction.
```

[^gender-simplicity]: This question assumes a gender binary for simplicity, though this binary does not reflect the true range of genders.
[^cond]: (Under special conditions we will outline later)
[^doc]: `for` loop tutorials from @r_for and @r_for_2; `sample` documentation from @r_sample, @r_sample_3, @r_sample_4, and @r_sample_2; `colon` operator documentation from @r_colon; `append` documentation from @r_append; 
[^refref]: Bibliography-making resources from @r_bib.


### Assumptions for using the normal model

As was mentioned earlier, there are a few assumptions we have to verify to use a normal distribution to model a sampling distribution.

Whether we are looking at a sampling distribution for a proportion or for a mean, me must always verify the following assumptions:

1. **Independence:** Each of the observations within a sample must be taken independently. This is often difficult (if not impossible) to check 
2. **Sample Size:** By the Central Limit Theorem, we must have a large enough sample size so that the normal model is a reasonable approximation.

The following conditions help us check these assumptions.

3. **Randomization:** If conducing an observational study, each observation in the sample should be chosen through random selection. If conducting an experiment, random assignment should be employed.  
4. **10% condition:** The sample size should be less than 10% of the population size

For a sampling distribution of proportions, we look at the following condition:

5. **Success/failure condition:** There should be at least 10 "successes" and 10 "failures," where "successes" and "failures" are just names for the two groups we're studying.

For a sampling distribution of means, we swap in the following condition:

5. **Size:** If the population is symmetric and unimodal, a smaller sample size works. However, if this is not the case, samples may need to be larger.

### Conclusions

In this article, we began by motivating sampling distributions. Sampling distributions are critical in enabling us to answer questions about populations using only samples. (After all, we don't expect all samples to be the same — that's sample variability). They allow us to view a theoretical distribution of all possible samples we could take in a population, which allows us to check how our sample stacks up. A crucial component is the famed **Central Limit Theorem**, which says our sampling distributions can be reasonably modeled after a normal distribution (given that special conditions are satisfied.)

### References[^refref]