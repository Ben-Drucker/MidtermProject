---
title: 'Sampling Distributions and Their Ample Uses!'
author: Ben Drucker
bibliography: bib.bib
csl: NumberedFormat.csl
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: show
    css: stat21-lab-theme.css
  pdf_document:
    toc: yes
    extra_dependencies: ["url"]
nocite: |
  @veaux2012stats
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidymodels)
library(patchwork)
library(dials)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
theme_set(theme_minimal(base_family = "Times"))
```

### Before Starting

Make sure to run the following code chunk to load in the correct libraries! For each question, the answer with relevant output is included in the "Answers" section. By default, each code chunk is shown. If you would like to hide a code chunk, click the "hide" button to the upper right of the chunk.
```{r}
library(tidyverse)
library(tidymodels)
library(patchwork)
library(dials)
library(ggplot2)
```

### Introduction and Motivation

Often, we are interested in statistical information for group(s) within a population. Usually, we focus on proportions or means of some aspect of the population. For example, in politics, we might wonder if the proportion of female gun-owners is 50% (i.e., is there an even gender split in American gun ownership[^gender-simplicity]?) Or, we might ask if there is a difference in the mean household income between men in opposite-sex marriages vs. in same-sex marriages.

For these types of questions, it would be completely impractical to survey every household. However, it *is* practical (and common) to survey a limited subset (**sample**) of the population in question. Obviously, it is unlikely that our sample provides the *exact* true answer. After all, we are not surveying the whole population! But what does it take to get an answer that is "close enough?" How many people do we need to survey? What assumptions about our data need to be met? Read on to find out!

### Sampling distributions for Proportions

Let's begin a simple example: marbles in a jar. Suppose I filled a jar filled with 10,000 blue marbles and 10,000 red marbles. Thus, there are twenty thousand marbles in total. However, suppose I don't tell you this, and I task you with estimating the proportion of blue marbles in the jar. It would be impractical to count all the marbles, so you decide to count a sample of them and use that as your estimate. (This is analogous to the example about the proportion of gun owners who are female!)

#### **Q1: The code below[^doc] simulates samples from the jar of size 3, 10, 100, and 1,000, and displays the results in histograms. Run this code multiple times. Before running this code, what do you predict will happen when sample size increases? After running the code, did you find your hypothesis was true? *Answer here: [Ans: Q1]***

```{r fig.show='hide'}
marbles <- c() # Initialize vector for the marbles

for (i in 1:10000) { # Add 10000 red marbles to the marbles vector
  marbles <- append(marbles, "red")
}

for (i in 1:10000) { # Add 10000 blue marbles to the marbles vector
  marbles <- append(marbles, "blue")
}

# For each size of sample, make a tibble dataframe of a sample.

size3 <- tibble(marbles3 = sample(marbles, 3))
size10 <- tibble(marbles10 = sample(marbles, 10))
size100 <- tibble(marbles100 = sample(marbles, 100))
size1000 <- tibble(marbles1000 = sample(marbles, 1000))

# Plot all four sample sizes using patchwork.
# To do this, the format is:

# ggplot(<our marble tibble>) +
#   geom_bar(...)                 [make a bar plot]
#   scale_fill_manual(...)        [specify custom fill values for the bars]
#   xlim(...)                     [specify the x labels we want]

# color values: make sure to assign the correct column the correct color!

color_values <- c("#0000ff6c", "#ff00006c")
no_blue_vals <- c("#ff00006c")

colors3 <- color_values
colors10 <- color_values
colors100 <- color_values
colors1000 <- color_values

if (length(pull(unique(size3["marbles3"]), "marbles3")) == 1) {
  colors3 <- no_blue_vals
}
if (length(pull(unique(size10["marbles10"]), "marbles10")) == 1) {
  colors10 <- no_blue_vals
}
if (length(pull(unique(size100["marbles100"]), "marbles100")) == 1) {
  colors100 <- no_blue_vals
}
if (length(pull(unique(size1000["marbles1000"]), "marbles1000")) == 1) {
  colors1000 <- no_blue_vals
}


marble_plot <- ggplot(size3) +
  geom_bar(mapping = aes(x = marbles3), fill = colors3) +
  xlim("blue", "red") +
  labs(x = "Color") +
  labs(y = "Count") +
  labs(title = "Simulated Sample of Size 3") +
ggplot(size10) +
  geom_bar(mapping = aes(x = marbles10), fill = colors10) +
  xlim("blue", "red") +
  labs(x = "Color") +
  labs(y = "Count") +
  labs(title = "Simulated Sample of Size 10") +
ggplot(size100) +
  geom_bar(mapping = aes(x = marbles100), fill = colors100) +
  xlim("blue", "red") +
  labs(x = "Color") +
  labs(y = "Count") +
  labs(title = "Simulated Sample of Size 100") +
ggplot(size1000) +
  geom_bar(mapping = aes(x = marbles1000), fill = colors1000) +
  xlim("blue", "red") +
  labs(x = "Color") +
  labs(y = "Count") +
  labs(title = "Simulated Sample of Size 1000")
```

What if we were to rerun this sampling procedure many times (with a fixed sample size), counted the proportion of blue marbles each time, and plotted the resulting proportions on a histogram? This would mean we wouldn't have to run the code above multiple times!

#### **Q2:** The following code chooses 1,000 samples from the marble jar, each of size 1000. What might you imagine this histogram of proportions of blue marbles in samples will look like? What will its center be? What shape will it be? Try running this code multiple times, with different sample sizes. What do you observe? *Answer here: [Ans: Q2]*

```{r fig.show='hide'}
v <- c()  # Initialize vector of sample proportion values

for (i in 1:1000) { # for each of 1000 samples, take a sample of size 1000
  size_ <- tibble(marbles_ = sample(marbles, 1000))
  filter(size_, marbles_ == "blue") %>%
    summarise(num_blue = n()) %>%
      select(num_blue) -> output # get the number of just blue marbles
  v <- append(v, output[[1]] / 1000) # append the sample proportion to v
}

data_ <- tibble(count = v) # make a tibble out of v

samp_dist <- ggplot(data_) + # make the plot
  geom_histogram(mapping = aes(x = count),
  binwidth = 0.005, color = "#eaee0594", fill = "#38c400b2") +
  xlim(0.4, 0.6) +
  labs(x = "Individual Sample Observations and Mean of Sample") +
  labs(y = "Counts of Individual Sample Means") +
  labs(title = "Sampling Distribution of 1000 Samples of Size 1000")
```

This type of histogram is called a **sampling distribution of a proportion**. This is because it is the *distribution* of *sample proportions* from which we are trying to gain information about a population.

The following fact about sampling distributions is possibly less obvious: as sample size increasesâ€”the *normal model* becomes a better and better model for the sampling distribution! This is due to the **Central Limit Theorem**, a pivotal result in statistics. (The fact specifically applying to sampling distributions is just one part of the more general result proven by Pierre-Simon Laplace in 1810.)

Now that we know the sampling distribution follows a normal model[^cond], it is natural to ask about its center and spread. 

#### **Q3:** What do you think the center of the sampling distribution is? (As a challenge, you can also think about its standard deviation, but it is difficult to guess just by looking at a sampling distribution.) *Answer here: [Ans: Q3]*

As mentioned earlier, standard deviation is trickier. To determine this quantity, we briefly step away from the normal model and turn to the underlying data. (Here, the "underlying data" is the set of all samples.) The data follows a binomial model[^repl_tech], which has standard deviation $\sqrt{np(1-p)}.$ This quantity is the standard deviation of the *number* of "successes" (one category in our population). However, we are interested in the standard deviation in the *proportion* of successes. Thus, we divide $\sqrt{np(1-p)}.$ by $n$ as follows:

$$\frac{\sqrt{np(1-p)}}{n} = \frac{\sqrt{np(1-p)}}{\sqrt{n^2}} = \sqrt{\frac{np(1-p)}{n^2}} = \sqrt{\frac{p(1-p)}{n}}.$$

(In this derivation, we are taking advantage of some more complicated facts about random variables.)

The center of the binomial distribution is the true number of successes. Thus, the center of the sampling distribution is the true number of successes divided by $n$, which equals $p$.

Now that we have the mean and standard deviation of the distribution, we can plot a normal curve over the simulated sampling distribution (from Q2):

```{r}
# Below, we plot a normal curve over our sampling distribution.
# (We multiply the normal curve by 5 because the area under of
# the histogram is 0.005 binwidth * 1000 points = 5
samp_dist_new <- samp_dist +
  stat_function(fun = function(x) 5 * dnorm(x, mean = mean(v),
  sd = sqrt(0.5 * 0.5 / 1000)), color = "#008321be",
  size = 2, alpha = 0.85, linetype = 5) +
  labs(title = "Sampling Distribution of 1000 Samples of Size 1000
  Including Normal Curve",
  x = "Proportion of Blue Marbles",
  y = "Number of Samples")

samp_dist_new
```

 

#### **Q4:** Calculate the mean and standard deviation for a sampling distribution of proportions with $p = 0.70$ and $n = 421$. *Answer here: [Ans: Q4]*

#### **Q5:** Suppose we think the proportion of blue marbles is 0.5. Would you be surprised if you took a sample of 1000 marbles and 51% were blue? What if 40% were blue? How might you quantify this answer? *Answer here: [Ans: Q5]*

The reason we care about sampling distributions is that they allow us to consider the distribution of sample proportions across many different (theoretical) samples. Then, we can compare this distribution to our actual sample and check how unlikely the proportion of our sample is *Given that our hypothesis---our null hypothesis---about the distribution's center is true*. This is called **hypothesis testing**. (This full topic is beyond the scope of this article, but it is important to get context about why sampling distributions are important.)

<!--TODO If time, go into detail about the SD of the binomial model-->

**Q6: Read the introduction to the following article on [gun ownership](https://news.gallup.com/poll/264932/percentage-americans-own-guns.aspx) (the study @pew about gun ownership mentioned earlier.) We want to examine the sampling distribution for this study. (Let your null hypothesis be "the proportion of gun owners who are female is 50%") First, draw out the sampling distribution indicating its center and standard deviation. Draw two vertical lines on the histogram corresponding to the sample proportion, $\hat{p}$. Qualitatively, what do you conclude?[^gun_ref]*Answer here: [Ans: Q6]***


### Sampling distributions for Means

It's great that we can create sampling distributions for proportions, but what do we do for numerical data? Not to worry! We can apply similar principles from the previous sections to means.

To see that the underlying principles of sampling distributions hold, consider the following example. Suppose we have 18000 marbles in a jar, but this time, the marbles' diameters are integers from 1 to 9 (inclusive), and there are 2000 marbles with each diameter. Thus, the population mean of the marble diameters is 5cm. Again, supposing we don't know this, we want to estimate the mean marble diameter using a sample. 

<!--FIXME: Replacement question -->

#### **Q7: The following code shows histograms of marble diameters found in individual random samples of differing sizes. It also shows the mean of the sample in the form of vertical red lines. Try running the code multiple times. Verify that the mean is less variable for samples of larger sizes. *Answer here: [Ans: Q7]***

```{r fig.show='hide'}
marbles <- c()
for (diam in 1:9) {
    for (i in 1:2000) {
    marbles <- append(marbles, diam)
  }
}
# TODO: make the histograms flush and with 10 bins
size3 <- tibble(marbles3 = sample(marbles, 3))
size10 <- tibble(marbles10 = sample(marbles, 10))
size100 <- tibble(marbles100 = sample(marbles, 100))
size1000 <- tibble(marbles1000 = sample(marbles, 1000))

size3 <- mutate(size3, mean_ = mean(marbles3))
size10 <- mutate(size10, mean_ = mean(marbles10))
size100 <- mutate(size100, mean_ = mean(marbles100))
size1000 <- mutate(size1000, mean_ = mean(marbles1000))

marble_plot_mean <- ggplot(size3) +
  geom_histogram(mapping = aes(x = marbles3), bins = 10,
  boundary = 0) +
  xlim(0, 10) +
  geom_vline(aes(xintercept = mean_), color = "red") +
  labs(x = "Individual Sample Observations and Mean of Sample") +
  labs(y = "Counts of Individual Sample Means") +
  labs(title = "Simulated Sample of Size 3") +
ggplot(size10) +
  geom_histogram(mapping = aes(x = marbles10), bins = 10,
  boundary = 0) +
  xlim(0, 10) +
  geom_vline(aes(xintercept = mean_), color = "red") +
  labs(x = "Individual Sample Observations and Mean of Sample") +
  labs(y = "Counts of Individual Sample Means") +
  labs(title = "Simulated Sample of Size 10") +
ggplot(size100) +
  geom_histogram(mapping = aes(x = marbles100), bins = 10,
  boundary = 0) +
  xlim(0, 10) +
  geom_vline(aes(xintercept = mean_), color = "red") +
  labs(x = "Individual Sample Observations and Mean of Sample") +
  labs(y = "Counts of Individual Sample Means") +
  labs(title = "Simulated Sample of Size 100") +
ggplot(size1000) +
  geom_histogram(mapping = aes(x = marbles1000), bins = 10,
  boundary = 0) +
  xlim(0, 10) +
  geom_vline(aes(xintercept = mean_), color = "red") +
  labs(x = "Individual Sample Observations and Mean of Sample") +
  labs(y = "Counts of Individual Sample Means") +
  labs(title = "Simulated Sample of Size 1000")
```

As we did before, let's now automate this process and create a histogram of means across many different samples.

```{r}

v <- c()

for (i in 1:1000) {
  sample_ <- sample(marbles, 1000)
  v <- append(v, mean(sample_))
}

data_ <- tibble(mean_ = v)

# Below, we plot a normal curve over our sampling distribution.
# (We multiply the normal curve by 25 because the area under of
# the histogram is 0.025 binwidth * 1000 points = 25

ggplot(data_) +
  geom_histogram(mapping = aes(x = mean_), binwidth = 0.025,
  color = "#eaee0594", fill = "#38c400b2") +
  xlim(4.5, 5.5) +
  stat_function(fun = function(x) 25 * dnorm(x, mean = mean(v),
  sd = sqrt(sd(marbles)**2 / 1000)), color = "#008321be",
  size = 2, alpha = 0.85, linetype = 5) + 
  labs(title = "Sampling Distribution of Means", 
  x = "Mean of Sample", y = "Number of Samples")
```

Yay! This distribution looks normal! (One amazing fact, again due to the Central Limit Theorem, is that *even if the population is not normally distributed*, as sample size increases, *the sampling distribution of means will be!*) What is its center and standard deviation? It should again feel natural the its center is $\mu$, the population mean. Just like before, standard deviation is also tricky. Here it is $\frac{\sigma}{\sqrt{n}}$, where $\sigma$ is the standard deviation of the *population*. In particular, just like the sampling distribution for proportions, note that the standard deviation is inversely proportion to the square root of sample size. This is why the means of samples of larger sizes vary less. (And the associated sampling distribution is narrower.)


<!--TODO: If time, explain reason for this standard deviation.-->

**Q8: Sometimes our sampling distribution is centered at zero. Why might this be? (Hint: What might we be trying to test with such a distribution) *Answer here: [Ans: Q8]***

[^gender-simplicity]: This question assumes a gender binary for simplicity, though this binary does not reflect the true range of genders.
[^cond]: (Under special conditions we will outline later)
[^doc]: `for` loop tutorials from @r_for and @r_for_2; `sample` documentation from @r_sample, @r_sample_3, @r_sample_4, and @r_sample_2; `colon` operator documentation from @r_colon; `append` documentation from @r_append; 
[^refref]: Bibliography-making resources from @r_bib.
[^gun_ref]: A tutorial for making the plot in the solution came from @r_normline
[^repl_tech]: Technically, since we are not replacing marbles, the data is not *exactly* binomial, but when we look at conditions for the normal model later, we see that this assumption is reasonable, as long as the sample size is small compared to the population. 

### Assumptions for using the normal model

As was mentioned earlier, there are a few assumptions we have to verify to use a normal distribution to model a sampling distribution.

Whether we are looking at a sampling distribution for a proportion or for a mean, me must always verify the following assumptions:

1. **Independence:** Each of the observations within a sample must be taken independently. This is often difficult (if not impossible) to check.
2. **Sample Size:** By the Central Limit Theorem, we must have a large enough sample size so that the normal model is a reasonable approximation.

The following conditions help us check these assumptions.

3. **Randomization:** If conducing an observational study, each observation in the sample should be chosen through random selection. If conducting an experiment, random assignment should be employed.  
4. **10% condition:** The sample size should be less than 10% of the population size. 

For a sampling distribution of proportions, we look at the following condition:

5. **Success/failure condition:** There should be at least 10 "successes" and 10 "failures," where "successes" and "failures" are just names for the two groups we're studying.

For a sampling distribution of means, we swap the following condition for the previous one:

5. **Size:** If the population is symmetric and unimodal, a smaller sample size works. However, if this is not the case, samples may need to be larger.

### Conclusions

In this lab activity, we began by motivating sampling distributions. Sampling distributions are critical in enabling us to answer questions about populations using only samples. (After all, we don't expect all samples to be the same â€” that's sample variability). They allow us to view a theoretical distribution of all possible samples we could take in a population, which allows us to check how our sample stacks up. A crucial component is the famed **Central Limit Theorem**, which says our sampling distributions can be reasonably modeled after a normal distribution (given that special conditions are satisfied.)

I hope you enjoyed this article!

### Answers

#### Ans: Q1
You should have found that as the sample size increases, the bar graphs are generally less variable (across different runs of the code) and closer to the true proportion of blue/red marbles in the jar (50%). Here is an example of one such run:

```{r echo=FALSE}
marble_plot
```

If this code was run multiple times, we would see that the histograms corresponding to larger samples vary less.

#### Ans: Q2
You should have observed that the distribution gets narrower and narrower as the sample size increases. This should intuitively make sense. As we increase sample size, it seems more likely that each sample is closer to the true proportion of blue marbles in the jar. (We also saw this in Q1!) Here is just one simulated sampling distribution:
```{r echo = FALSE}
samp_dist
```

#### Ans: Q3

As you can probably imagine, sampling distributions for proportions are centered at $p$, the true population proportion. This should intuitively make sense; with many random samples, we would expect about half the proportions to be less than the true population proportion and half to be more.

#### Ans: Q4
The answer is as follows:

$$\text{mean} = 0.70$$
$$\text{SD} = \sqrt{\frac{0.7\cdot0.3}{421}} \approx 0.0223$$

#### Ans: Q5

Let's think about the underlying sampling distribution.
It is centered at the true proportion of blue marbles.
Since we suppose that $p = 0.5$, the standard deviation of
the sampling distribution is $\sqrt{\frac{0.5\cdot0.5}{1000}} \approx 0.0158$.
Thus, if we picked 51% blue marbles in our sample, the 1%
increase over $p$ represents $\frac{0.01}{0.0158} \approx 0.632$ SDs above $p$.
How surprising is this? The probability of getting a difference of
0.632 SDs above or below the mean of the distribution ($p$) is the
area under the bell curve to the left and right of 0.632 SDs above
and below the mean. Using R code, this probability is 
\verb|2*pnorm(-0.632)|, or about 52.7%. So getting 51% is not 
that surprising. On the other hand, if we do a similar analysis with 
the 40% case, the probability drops all the way to 0.00000000764%!
So this would be extremely surprising. 

#### Ans: Q6
From the article, 62% of gun owners are male 
(which, for simplicity means that 38% of gun owners are female[^gender-simplicity].)
Since our assumption was that there is no gender gap in gun ownership,
our sampling distribution is centered at 0.5, and has a standard
deviation of $\sqrt{\frac{0.5*0.5}{1269}} \approx 0.014$ since the sample size was 1296. In the graph, the y axis represents relative number of samples which guarantees area equals 1 under the normal curve.
Graph below:
```{r}

ggplot(data = data.frame(x = c(0.35, 0.65)), mapping =  aes(x = x)) +
  stat_function(fun = dnorm, args = list(0.5, 0.014)) +
  geom_vline(xintercept = 0.38, color = "red") +
  geom_vline(xintercept = 0.62, color = "red") +
  xlab("Proportion of Female Gun Owners") +
  ylab("Relative Number of Samples")

```

Based on the graph, it is very unlikely that our
sample proportion of 38% (or more extreme) would have been chosen, given that
we assume the true proportion of female gun owners is 50%.

#### Ans: Q7
One resultant plot:
```{r}
marble_plot_mean
```

If this code was run multiple times, we would see that means of the histograms corresponding to larger samples vary less.

#### Ans: Q8

Our sampling distribution would be centered at zero if we
are hypothesis testing a difference between two means or two proportions,
and we hypothesize that the true difference is zero.
One example is the study about incomes in marriages with different gender
configurations mentioned in the introduction.

### References[^refref]